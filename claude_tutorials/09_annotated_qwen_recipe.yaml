# ==============================================================================
# ANNOTATED: Qwen2 7B LoRA Recipe - Different Tokenization Approach
# ==============================================================================
# Source: recipes/configs/qwen2/7B_lora.yaml
#
# **WHAT**: LoRA fine-tuning for Alibaba's Qwen2 7B model
#
# **WHY THIS EXAMPLE MATTERS**: Demonstrates tokenizer flexibility!
#           - Llama/Gemma: SentencePiece tokenization
#           - Qwen: BPE tokenization (GPT-2 style)
#           - Same recipe handles both seamlessly
#
# **QWEN vs. LLAMA vs. GEMMA**:
#
#   Tokenization:
#     - Llama/Gemma: SentencePiece (unigram or BPE variant)
#     - Qwen: Byte-Pair Encoding (BPE) like GPT-2/GPT-3
#
#   Files needed:
#     - Llama/Gemma: tokenizer.model (single file)
#     - Qwen: vocab.json + merges.txt (two files)
#
#   Vocabulary:
#     - Llama 3: 128K tokens
#     - Gemma: 256K tokens
#     - Qwen2: 151K tokens (optimized for Chinese + English)
#
#   Special tokens:
#     - All: <|begin_of_text|>, <|end_of_text|>, etc.
#     - Qwen: Additional tokens for function calling
#
# **KEY DIFFERENCES FROM OTHER RECIPES**:
#   1. Tokenizer: BPE (2 files) vs SentencePiece (1 file)
#   2. Model builder: qwen2 vs llama3_1 vs gemma
#   3. Gradient accumulation: 8 (higher) for effective batch of 16
#   4. Activation checkpointing: Disabled (different memory strategy)
#   5. Origin: Alibaba vs Meta vs Google
#
# **RECIPE ENTRY POINT**: recipes/lora_finetune_distributed.py (same as others!)
# ==============================================================================

output_dir: /tmp/torchtune/qwen2_7B/lora

# ==============================================================================
# Model: Qwen2 7B with LoRA
# ==============================================================================
# **QWEN2 ARCHITECTURE**:
#   - Similar to Llama 3: Decoder-only transformer
#   - Uses GQA (Grouped Query Attention)
#   - RoPE for positional encoding
#   - SwiGLU activation (FFN)
#   - RMSNorm for normalization
#
# **QWEN MODEL FAMILY**:
#   - Qwen2: 0.5B, 1.5B, 7B, 72B
#   - Qwen2.5: Improved versions
#   - Qwen-VL: Vision-language models
#   - Optimized for multilingual (Chinese + English)
#
# **LORA CONFIGURATION**: Same as Llama 8B (rank=8, alpha=16)
# ==============================================================================
model:
  _component_: torchtune.models.qwen2.lora_qwen2_7b  # ← Qwen2 model family
  lora_attn_modules: ['q_proj', 'v_proj', 'output_proj']
  apply_lora_to_mlp: True
  apply_lora_to_output: False
  lora_rank: 8  # Same as Llama 8B (not as high as Gemma 2B's 64)
  lora_alpha: 16  # 2× rank
  lora_dropout: 0.0

# ==============================================================================
# Tokenizer: BPE (CRITICAL DIFFERENCE!)
# ==============================================================================
# **BPE TOKENIZATION EXPLAINED**:
#
# Unlike SentencePiece (Llama/Gemma), Qwen uses Byte-Pair Encoding:
#
# 1. **Two Files Required**:
#    - vocab.json: Maps tokens to IDs {"hello": 0, "world": 1, ...}
#    - merges.txt: Defines merge rules for byte pairs
#
# 2. **How BPE Works**:
#    ```
#    Input: "hello world"
#
#    Step 1: Split into bytes
#    → ['h', 'e', 'l', 'l', 'o', ' ', 'w', 'o', 'r', 'l', 'd']
#
#    Step 2: Apply merge rules (from merges.txt)
#    → ['h', 'e', 'll', 'o', ' ', 'w', 'o', 'r', 'l', 'd']  # 'll' merged
#    → ['hello', ' ', 'world']  # More merges
#
#    Step 3: Map to IDs (from vocab.json)
#    → [15496, 995]
#    ```
#
# 3. **vs. SentencePiece (Llama/Gemma)**:
#    - BPE: Deterministic merges, byte-level fallback
#    - SentencePiece: Probabilistic (unigram) or BPE variant
#    - BPE: Common in GPT models (GPT-2, GPT-3)
#    - SentencePiece: Common in T5, mT5, Llama
#
# 4. **Multilingual Optimization**:
#    Qwen's vocab optimized for Chinese characters:
#    - Chinese characters: Single tokens (efficient)
#    - English words: Multiple tokens (BPE subwords)
#    - Example: "你好世界" → [101, 102, 103, 104] (4 tokens)
#              "hello world" → [15496, 995] (2 tokens)
#
# **IMPLEMENTATION**: torchtune/models/qwen2/_tokenizer.py
#                     Uses HuggingFace tokenizers library (fast Rust impl)
# ==============================================================================
tokenizer:
  _component_: torchtune.models.qwen2.qwen2_tokenizer  # ← BPE tokenizer!
  path: /tmp/Qwen2-7B-Instruct/vocab.json  # ← Token → ID mapping
  merges_file: /tmp/Qwen2-7B-Instruct/merges.txt  # ← BPE merge rules
  max_seq_len: null  # Use model's default (typically 32K for Qwen2)

# ==============================================================================
# Checkpointer: Qwen2 Format
# ==============================================================================
# **CHECKPOINT STRUCTURE**: 4 files for 7B model (same as Llama 8B)
#                           Qwen2 7B ≈ Llama 8B in size
#
# **MODEL TYPE**: QWEN2 (affects weight key mapping)
#                 Different naming conventions than Llama/Gemma
# ==============================================================================
checkpointer:
  _component_: torchtune.training.FullModelHFCheckpointer
  checkpoint_dir: /tmp/Qwen2-7B-Instruct
  checkpoint_files: [
    model-00001-of-00004.safetensors,
    model-00002-of-00004.safetensors,
    model-00003-of-00004.safetensors,
    model-00004-of-00004.safetensors
  ]
  recipe_checkpoint: null
  output_dir: ${output_dir}
  model_type: QWEN2  # ← Qwen-specific weight mapping

resume_from_checkpoint: False

# ==============================================================================
# Dataset: Same across all model families!
# ==============================================================================
# **PORTABILITY**: Dataset works with any tokenizer
#                  - Llama tokenizer: 128K vocab
#                  - Gemma tokenizer: 256K vocab
#                  - Qwen tokenizer: 151K vocab
#                  All produce different token IDs, but same semantics
# ==============================================================================
dataset:
  _component_: torchtune.datasets.alpaca_cleaned_dataset
  packed: False
  split: train[:95%]

seed: null
shuffle: True

# ==============================================================================
# Validation Dataset
# ==============================================================================
run_val_every_n_steps: null
dataset_val:
  _component_: torchtune.datasets.alpaca_cleaned_dataset
  split: train[95%:]
batch_size_val: ${batch_size}

# ==============================================================================
# Optimizer: Same as Llama!
# ==============================================================================
# **LEARNING RATE**: 3e-4 (same as Llama, higher than Gemma)
#                    Qwen and Llama have similar architecture depth
#
# **WEIGHT DECAY**: 0.01 (standard L2 regularization)
# ==============================================================================
optimizer:
  _component_: torch.optim.AdamW
  fused: True
  weight_decay: 0.01
  lr: 3e-4  # Same as Llama 8B LoRA

lr_scheduler:
  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup
  num_warmup_steps: 100

# ==============================================================================
# Loss: Model-Agnostic
# ==============================================================================
loss:
  _component_: torchtune.modules.loss.LinearCrossEntropyLoss

# ==============================================================================
# Training Configuration
# ==============================================================================
epochs: 1
batch_size: 2  # Per-GPU batch size
max_steps_per_epoch: null

# **HIGH GRADIENT ACCUMULATION**: 8 steps
#   - Effective batch size: 2 × 2 GPUs × 8 accum = 32
#   - Trades speed for larger effective batch (better gradients)
#   - Memory: Same as batch_size=2
gradient_accumulation_steps: 8  # Higher than other recipes!

clip_grad_norm: null
compile: False

# ==============================================================================
# Logging
# ==============================================================================
metric_logger:
  _component_: torchtune.training.metric_logging.DiskLogger
  log_dir: ${output_dir}/logs

log_every_n_steps: 1
log_peak_memory_stats: True
log_level: INFO

# ==============================================================================
# Environment & Memory
# ==============================================================================
device: cuda
dtype: bf16

# **ACTIVATION CHECKPOINTING DISABLED**: Different memory strategy than Gemma
#   - Why disabled? 7B model fits comfortably on 2 GPUs with LoRA
#   - Trade-off: 10% faster training, slightly more memory
#   - If OOM: Enable this (sacrifices speed for memory)
enable_activation_checkpointing: False  # ← Disabled (vs True for Gemma)
enable_activation_offloading: False

# ==============================================================================
# Profiler (Same as Gemma)
# ==============================================================================
profiler:
  _component_: torchtune.training.setup_torch_profiler
  enabled: False
  output_dir: ${output_dir}/profiling_outputs
  cpu: True
  cuda: True
  profile_memory: False
  with_stack: False
  record_shapes: True
  with_flops: False
  wait_steps: 5
  warmup_steps: 5
  active_steps: 2
  num_cycles: 1

# ==============================================================================
# TOKENIZATION DEEP DIVE: BPE vs. SentencePiece
# ==============================================================================
"""
**WHY DIFFERENT TOKENIZERS?**

Historical Context:
- SentencePiece (Google, 2018): Unified library for subword tokenization
  - Used in: T5, mT5, BERT variants, Llama, Gemma
  - Advantages: Language-agnostic, handles unknown chars well
  - Training: Unigram LM or BPE variant

- BPE (OpenAI, 2016): Byte-Pair Encoding
  - Used in: GPT-2, GPT-3, GPT-4, Qwen
  - Advantages: Simple, deterministic, byte-level fallback
  - Training: Greedy merge algorithm

**CONCRETE EXAMPLE**:

Input Text: "Qwen是Alibaba的大模型"  (Qwen is Alibaba's large model)

**Qwen Tokenizer (BPE)**:
```python
tokens = ["Qwen", "是", "Alibaba", "的", "大", "模型"]
ids = [48, 1345, 2879, 1389, 1456, 2341]
# Chinese characters: Single tokens (efficient!)
```

**Llama Tokenizer (SentencePiece)**:
```python
tokens = ["Q", "wen", "▁是", "▁Ali", "baba", "▁的", "▁大", "▁模型"]
ids = [48, 12453, 29871, 31882, 28618, 30691, 31360, 32845]
# Chinese: Less efficient (not optimized for Chinese)
```

**TOKENIZER FILES BREAKDOWN**:

1. **vocab.json** (Qwen):
   ```json
   {
     "!": 0,
     "\"": 1,
     "#": 2,
     ...
     "hello": 15496,
     "world": 995,
     "你好": 101,
     ...
   }
   ```
   Total: ~151,643 entries

2. **merges.txt** (Qwen):
   ```
   #version: 0.2
   Ġ t
   Ġ a
   h e
   ...
   ```
   Each line: Two tokens to merge
   Order matters: Earlier = higher priority

3. **tokenizer.model** (Llama - SentencePiece):
   Binary protobuf file containing:
   - Vocabulary
   - Merge scores
   - Normalization rules
   - Special tokens

**IMPLEMENTATION IN TORCHTUNE**:

```python
# torchtune/models/qwen2/_tokenizer.py
from transformers import PreTrainedTokenizerFast

class Qwen2Tokenizer:
    def __init__(self, path: str, merges_file: str):
        self.tokenizer = PreTrainedTokenizerFast(
            vocab_file=path,  # vocab.json
            merges_file=merges_file,  # merges.txt
            # Special tokens
            bos_token="<|im_start|>",
            eos_token="<|im_end|>",
        )

    def encode(self, text: str) -> List[int]:
        return self.tokenizer.encode(text)

    def decode(self, ids: List[int]) -> str:
        return self.tokenizer.decode(ids)
```

vs.

```python
# torchtune/models/llama3/_tokenizer.py
from sentencepiece import SentencePieceProcessor

class Llama3Tokenizer:
    def __init__(self, path: str):
        self.sp_model = SentencePieceProcessor(model_file=path)

    def encode(self, text: str) -> List[int]:
        return self.sp_model.encode(text)

    def decode(self, ids: List[int]) -> str:
        return self.sp_model.decode(ids)
```

**KEY INSIGHT**:
- Different tokenizers, SAME INTERFACE
- Dataset builders don't care which tokenizer
- Recipe code is tokenizer-agnostic
- Only config needs to change!

**MULTILINGUAL COMPARISON**:

Text: "The cat sat on the mat. 猫坐在垫子上。"

Qwen (151K vocab, optimized for Chinese):
→ 15 tokens total (8 English + 7 Chinese)

Llama (128K vocab, English-centric):
→ 24 tokens total (8 English + 16 Chinese)

Gemma (256K vocab, multilingual):
→ 18 tokens total (8 English + 10 Chinese)

**QWEN MODEL FAMILY OVERVIEW**:

┌────────────────┬──────────┬───────────┬─────────────┬──────────────┐
│ Model          │ Params   │ Context   │ Languages   │ Use Case     │
├────────────────┼──────────┼───────────┼─────────────┼──────────────┤
│ Qwen2 0.5B     │ 0.5B     │ 32K       │ Multi       │ Edge devices │
│ Qwen2 1.5B     │ 1.5B     │ 32K       │ Multi       │ Mobile       │
│ Qwen2 7B       │ 7B       │ 32K       │ Multi       │ General      │
│ Qwen2 72B      │ 72B      │ 32K       │ Multi       │ High-quality │
│ Qwen2.5 7B     │ 7B       │ 128K      │ Multi       │ Long context │
│ Qwen-VL 7B     │ 7B       │ 8K        │ Multi+Vision│ Multimodal   │
└────────────────┴──────────┴───────────┴─────────────┴──────────────┘

**GRADIENT ACCUMULATION STRATEGY**:

Why gradient_accumulation_steps=8 for Qwen (vs 1 for Gemma)?

```python
# Without gradient accumulation:
for batch in dataloader:  # batch_size=2
    loss = model(batch)
    loss.backward()
    optimizer.step()  # Update every 2 samples
    optimizer.zero_grad()

# With gradient accumulation=8:
for i, batch in enumerate(dataloader):  # batch_size=2
    loss = model(batch)
    loss = loss / 8  # Scale loss
    loss.backward()  # Accumulate gradients

    if (i + 1) % 8 == 0:
        optimizer.step()  # Update every 16 samples (2×8)
        optimizer.zero_grad()
```

Benefits:
- Effective batch size: 2 × 2 GPUs × 8 = 32
- Memory usage: Same as batch_size=2
- Gradient quality: Better (larger effective batch)
- Throughput: 12% slower (more backward passes per update)

**ACTIVATION CHECKPOINTING DECISION**:

Why disabled for Qwen 7B LoRA?

Memory Calculation:
```
Model Weights:      7B × 2 bytes (bf16) = 14 GB
LoRA Adapters:      ~20M × 2 bytes = 0.04 GB
Optimizer States:   2× LoRA = 0.08 GB
Activations:        Batch × Seq × Hidden × Layers
                    = 2 × 2048 × 4096 × 28
                    = ~4 GB (without checkpointing)
                    = ~0.5 GB (with checkpointing)

Total per GPU (2 GPUs):
- Without checkpointing: ~9 GB
- With checkpointing: ~5.5 GB

With 16GB or 24GB GPUs → Checkpointing not needed!
```

Trade-off:
- Checkpointing enabled: 10-15% slower, saves ~3.5 GB
- Checkpointing disabled: Faster, uses more memory

For production with limited GPU memory → enable checkpointing
For research with ample memory → disable for speed

**CROSS-MODEL RECIPE COMPATIBILITY MATRIX**:

┌──────────────┬────────────┬────────────┬─────────────┬─────────────┐
│ Component    │ Llama 3.1  │ Gemma 2B   │ Qwen2 7B    │ Compatible? │
├──────────────┼────────────┼────────────┼─────────────┼─────────────┤
│ Recipe .py   │ ✓          │ ✓          │ ✓           │ YES         │
│ Dataset      │ ✓          │ ✓          │ ✓           │ YES         │
│ Optimizer    │ ✓          │ ✓          │ ✓           │ YES         │
│ Loss         │ ✓          │ ✓          │ ✓           │ YES         │
│ Model        │ llama3_1   │ gemma      │ qwen2       │ NO (config) │
│ Tokenizer    │ SP (1 file)│ SP (1 file)│ BPE (2 file)│ NO (config) │
│ Checkpointer │ LLAMA3     │ GEMMA      │ QWEN2       │ NO (config) │
└──────────────┴────────────┴────────────┴─────────────┴─────────────┘

**SUMMARY**:
This Qwen recipe showcases:
- ✅ BPE tokenization (different from SentencePiece)
- ✅ Two-file tokenizer (vocab.json + merges.txt)
- ✅ Multilingual optimization (Chinese + English)
- ✅ High gradient accumulation (effective batch size)
- ✅ Activation checkpointing disabled (speed over memory)
- ✅ Same recipe code as Llama/Gemma (config-driven!)

Perfect demonstration of tokenizer flexibility and modularity!
"""

